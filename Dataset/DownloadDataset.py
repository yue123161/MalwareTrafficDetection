
import ssl
from bs4 import BeautifulSoup
import requests
import os
import urllib

def find_files(url):
    soup = BeautifulSoup(requests.get(url, verify=False).text, "lxml")
    hrefs = []
    for a in soup.find_all('a'):
        try:
            hrefs.append(a['href'])
        except:
            pass
    return hrefs

def download_files(url, datasetname):
    local_path = "D:\\Work\\PyCharm-workspace\\MalwareTrafficDetection\\Dataset\\"

    os.mkdir(local_path+datasetname[:-1])
    os.mkdir(local_path+datasetname[:-1]+"\\bro")

    urllib.request.urlretrieve(url + datasetname + "/bro/conn.log", local_path+datasetname[:-1]+"\\bro\\conn.log")
    # u = urllib3.open(url + datasetname + "/bro/conn.log")
    # f = open(local_path+datasetname[:-1]+"\\bro\\conn.log")
    # block_sz = 8192
    # while True:
    #     buffer = u.read(block_sz)
    #     if not buffer:
    #         break
    #     f.write(buffer)
    # f.close()
    print("finish downloading "+ datasetname[:-2])

ssl._create_default_https_context = ssl._create_unverified_context
url = "https://mcfp.felk.cvut.cz/publicDatasets/"

# #find the urls to download
# names = []
# hrefs = find_files(url)
# for href in hrefs:
#     if "CTU-Malware-Capture-Botnet" in href or "CTU-Mixed-Capture" in href or "CTU-Normal" in href:
#         names.append(href)
# print(len(names),": ", names)
# #delete the datasets which do not have ssl.log
# dataset_names = []
# for name in names:
#     files = find_files(url+name+"bro/")
#     if 'ssl.log' in files:
#         dataset_names.append(name)
# print(len(dataset_names),dataset_names)
# #find local dataset that have been downloaded
# path = "D:\\Work\\PyCharm-workspace\\MalwareTrafficDetection\\Dataset"
# downloaded_datasets = os.listdir(path)
# #download needed files
# for dname in dataset_names:
#     if dname[:-2] not in downloaded_datasets:
#         download_files(url, dname)
download_files(url, "CTU-Malware-Capture-Botnet-129-1/")